# ğŸ¤– IA & Machine Learning

## VisÃ£o Geral

O mÃ³dulo de InteligÃªncia Artificial do Sistema de GestÃ£o Financeira Pessoal utiliza tÃ©cnicas avanÃ§adas de Machine Learning para fornecer insights preditivos, detecÃ§Ã£o de anomalias, categorizaÃ§Ã£o automÃ¡tica e recomendaÃ§Ãµes personalizadas, transformando dados financeiros em inteligÃªncia acionÃ¡vel.

## Ãndice

1. [Arquitetura de IA](#arquitetura-de-ia)
2. [Modelos Implementados](#modelos-implementados)
3. [Pipeline de Dados](#pipeline-de-dados)
4. [Feature Engineering](#feature-engineering)
5. [Treinamento e AvaliaÃ§Ã£o](#treinamento-e-avaliaÃ§Ã£o)
6. [Deployment e Serving](#deployment-e-serving)
7. [Monitoramento e Retreinamento](#monitoramento-e-retreinamento)
8. [APIs de IA](#apis-de-ia)
9. [Casos de Uso](#casos-de-uso)
10. [Roadmap de IA](#roadmap-de-ia)

---

## Arquitetura de IA

### Stack TecnolÃ³gica

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CAMADA DE APLICAÃ‡ÃƒO                       â”‚
â”‚  Backend Java (Spring Boot) + Frontend React                 â”‚
â”‚  Consome APIs de IA via HTTP REST                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ HTTP/REST
                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AI SERVICE (FastAPI)                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              API Layer (FastAPI)                     â”‚  â”‚
â”‚  â”‚  /predict/expenses  /detect/anomalies               â”‚  â”‚
â”‚  â”‚  /classify/category /recommend/savings               â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                   â”‚                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚           Service Layer (Business Logic)             â”‚  â”‚
â”‚  â”‚  - Preprocessing  - Model Selection                  â”‚  â”‚
â”‚  â”‚  - Postprocessing - Result Formatting                â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                   â”‚                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              Model Layer                             â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  â”‚  Predictor   â”‚  â”‚   Detector   â”‚  â”‚ Classifierâ”‚  â”‚  â”‚
â”‚  â”‚  â”‚  (RF/XGB)    â”‚  â”‚  (Iso Forest)â”‚  â”‚ (NB/BERT) â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚           Data Layer (Feature Store)                 â”‚  â”‚
â”‚  â”‚  - Raw Features  - Engineered Features              â”‚  â”‚
â”‚  â”‚  - Cache Layer   - Vector Store                      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DATA SOURCES                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚  PostgreSQL  â”‚  â”‚     Redis    â”‚  â”‚   S3/Minio   â”‚       â”‚
â”‚  â”‚  (TransaÃ§Ãµes)â”‚  â”‚   (Cache)    â”‚  â”‚   (Models)   â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Tecnologias Utilizadas

**Core ML:**
- Python 3.11+
- Scikit-learn 1.3+
- TensorFlow 2.14+ (para deep learning futuro)
- XGBoost 2.0+
- LightGBM 4.0+

**NLP:**
- spaCy 3.7+ (processamento de texto)
- Transformers 4.35+ (BERT para categorizaÃ§Ã£o)
- NLTK (tokenizaÃ§Ã£o e stemming)

**Data Processing:**
- Pandas 2.1+ (manipulaÃ§Ã£o de dados)
- NumPy 1.26+ (operaÃ§Ãµes numÃ©ricas)
- Dask (processamento paralelo para grandes volumes)

**API Framework:**
- FastAPI 0.104+ (endpoints)
- Pydantic 2.5+ (validaÃ§Ã£o)
- Uvicorn (ASGI server)

**ML Ops:**
- MLflow (tracking e registry)
- DVC (versionamento de dados e modelos)
- Prometheus + Grafana (monitoring)

---

## Modelos Implementados

### 1. Preditor de Despesas

#### Objetivo
Prever o valor total de despesas para os prÃ³ximos meses com base em histÃ³rico e padrÃµes.

#### Algoritmo
**Random Forest Regressor** com hiperparÃ¢metros otimizados

**Por que Random Forest?**
- Robusto a outliers
- Captura relaÃ§Ãµes nÃ£o-lineares
- Feature importance intrÃ­nseca
- Baixo risco de overfitting
- Bom desempenho com dados tabulares

#### Features Utilizadas

**Temporais:**
```python
features_temporais = {
    'mes': int,                    # 1-12
    'ano': int,
    'dia_mes': int,                # 1-31
    'dia_semana': int,             # 0-6 (seg-dom)
    'quinzena': int,               # 1 ou 2
    'dias_uteis_mes': int,
    'is_feriado': bool,
    'is_fim_semana': bool,
}
```

**Agregadas (Ãºltimos N perÃ­odos):**
```python
features_agregadas = {
    'media_despesas_3m': float,
    'media_despesas_6m': float,
    'media_despesas_12m': float,
    'desvio_padrao_3m': float,
    'tendencia_3m': float,         # slope da regressÃ£o linear
    'sazonalidade': float,          # Ã­ndice sazonal
    'max_mes_anterior': float,
    'min_mes_anterior': float,
}
```

**CategÃ³ricas:**
```python
features_categoricas = {
    'categoria_top_1': str,        # Categoria com mais gastos
    'categoria_top_2': str,
    'categoria_top_3': str,
    'num_categorias_ativas': int,
}
```

**Contextuais:**
```python
features_contextuais = {
    'total_receitas_mes': float,
    'salario': float,
    'num_despesas_mes_anterior': int,
    'percentual_parceladas': float,
    'percentual_recorrentes': float,
}
```

#### HiperparÃ¢metros

```python
random_forest_params = {
    'n_estimators': 200,           # NÃºmero de Ã¡rvores
    'max_depth': 15,               # Profundidade mÃ¡xima
    'min_samples_split': 10,       # Min amostras para split
    'min_samples_leaf': 4,         # Min amostras nas folhas
    'max_features': 'sqrt',        # Features por split
    'bootstrap': True,
    'oob_score': True,             # Out-of-bag score
    'random_state': 42,
    'n_jobs': -1,                  # Usar todos os cores
}
```

#### Performance

| MÃ©trica | Valor | DescriÃ§Ã£o |
|---------|-------|-----------|
| MAE | R$ 180 | Erro absoluto mÃ©dio |
| RMSE | R$ 245 | Root mean squared error |
| MAPE | 8.5% | Mean absolute percentage error |
| RÂ² | 0.87 | Coeficiente de determinaÃ§Ã£o |

**InterpretaÃ§Ã£o:**
- Modelo erra em mÃ©dia R$ 180 nas previsÃµes
- 87% da variÃ¢ncia Ã© explicada pelo modelo
- Performance considerada **excelente** para dados financeiros

#### Intervalo de ConfianÃ§a

```python
def predict_with_confidence(model, X, confidence=0.80):
    """
    Retorna previsÃ£o com intervalo de confianÃ§a
    """
    predictions = []
    for tree in model.estimators_:
        predictions.append(tree.predict(X))
    
    predictions = np.array(predictions)
    mean = predictions.mean(axis=0)
    std = predictions.std(axis=0)
    
    # Intervalo de confianÃ§a 80%
    lower = mean - 1.28 * std  # 10Âº percentil
    upper = mean + 1.28 * std  # 90Âº percentil
    
    return {
        'valor_previsto': mean,
        'intervalo_min': lower,
        'intervalo_max': upper,
        'confianca': confidence
    }
```

---

### 2. Detector de Anomalias

#### Objetivo
Identificar despesas atÃ­picas que fogem do padrÃ£o histÃ³rico do usuÃ¡rio.

#### Algoritmo
**Isolation Forest** + **Z-Score** combinados

**Por que Isolation Forest?**
- Projetado especificamente para detecÃ§Ã£o de anomalias
- NÃ£o requer distribuiÃ§Ã£o normal dos dados
- Eficiente computacionalmente
- Funciona bem com alta dimensionalidade

#### Features Utilizadas

```python
features_anomaly = {
    # Valor normalizado
    'valor_normalizado': float,           # (valor - mean) / std
    'valor_log': float,                   # log(valor + 1)
    
    # Desvios
    'desvio_media_categoria': float,      # valor - mÃ©dia_categoria
    'desvio_media_usuario': float,        # valor - mÃ©dia_usuario
    'percentil_categoria': float,         # 0-100
    
    # Temporais
    'hora_transacao': int,                # 0-23
    'dia_semana': int,                    # 0-6
    'is_horario_incomum': bool,           # 22h-6h
    
    # Contextuais
    'percentual_receita': float,          # valor / receita_mensal
    'diferenca_transacao_anterior': float,
    'frequencia_categoria_mes': int,
    
    # Comportamentais
    'num_transacoes_dia': int,
    'velocidade_gasto': float,            # R$/hora
}
```

#### Processo de DetecÃ§Ã£o

```python
def detect_anomaly(transaction, user_history):
    """
    Pipeline de detecÃ§Ã£o multi-mÃ©todo
    """
    # 1. Isolation Forest Score
    iso_score = isolation_forest.decision_function([features])[0]
    iso_anomaly = iso_score < -0.3  # Threshold calibrado
    
    # 2. Z-Score (para valores extremos)
    z_score = (valor - mean) / std
    z_anomaly = abs(z_score) > 3
    
    # 3. Percentil (categoria especÃ­fica)
    percentil = percentileofscore(category_history, valor)
    percentil_anomaly = percentil > 95  # Top 5%
    
    # 4. Regras de negÃ³cio
    rule_anomaly = (
        valor > receita_mensal * 0.5 or  # > 50% da receita
        valor > mean * 5 or              # 5x a mÃ©dia
        is_horario_incomum               # Madrugada
    )
    
    # AgregaÃ§Ã£o (qualquer mÃ©todo detecta â†’ anomalia)
    is_anomaly = iso_anomaly or z_anomaly or percentil_anomaly or rule_anomaly
    
    # Score combinado (0-1)
    combined_score = (
        0.4 * abs(iso_score) +
        0.3 * min(abs(z_score) / 3, 1) +
        0.2 * (percentil / 100) +
        0.1 * (1 if rule_anomaly else 0)
    )
    
    return {
        'is_anomaly': is_anomaly,
        'score': combined_score,
        'confidence': get_confidence(combined_score),
        'reasons': get_anomaly_reasons(...)
    }
```

#### Thresholds Calibrados

| MÃ©todo | Threshold | DescriÃ§Ã£o |
|--------|-----------|-----------|
| Isolation Forest | -0.3 | Score de isolamento |
| Z-Score | 3.0 | Desvios padrÃ£o |
| Percentil | 95% | Percentil da categoria |
| Valor | 50% receita | Percentual da renda |

#### Tipos de Anomalias Detectadas

1. **Valor Extremo**
   - Despesa muito acima da mÃ©dia histÃ³rica
   - Ex: Compra de R$ 5.000 quando mÃ©dia Ã© R$ 300

2. **PadrÃ£o Temporal Incomum**
   - TransaÃ§Ã£o em horÃ¡rio atÃ­pico
   - Ex: Compra Ã s 3h da manhÃ£

3. **FrequÃªncia Anormal**
   - Muitas transaÃ§Ãµes em curto perÃ­odo
   - Ex: 10 compras em 1 hora

4. **Categoria Incomum**
   - Gasto em categoria rara para o usuÃ¡rio
   - Ex: Primeira compra em "Joias" em 2 anos

5. **LocalizaÃ§Ã£o Suspeita** (futuro)
   - TransaÃ§Ã£o em local diferente do usual
   - Requer integraÃ§Ã£o com dados de localizaÃ§Ã£o

#### Performance

| MÃ©trica | Valor |
|---------|-------|
| Precision | 85% |
| Recall | 78% |
| F1-Score | 81% |
| False Positive Rate | 5% |

**InterpretaÃ§Ã£o:**
- 85% das anomalias detectadas sÃ£o verdadeiras
- 78% das verdadeiras anomalias sÃ£o detectadas
- Taxa de falso positivo aceitÃ¡vel (5%)

---

### 3. Classificador de Categorias

#### Objetivo
Categorizar automaticamente despesas baseado na descriÃ§Ã£o.

#### Algoritmo
**Multinomial Naive Bayes** + **TF-IDF** (fase 1)  
**BERT Fine-tuned** (fase 2 - futuro)

**Por que Naive Bayes inicialmente?**
- Simples e interpretÃ¡vel
- RÃ¡pido para treinar e inferir
- Bom desempenho em classificaÃ§Ã£o de texto
- Baixo custo computacional

#### Pipeline de Processamento

```python
# 1. PrÃ©-processamento
def preprocess_text(descricao):
    # Lowercasing
    text = descricao.lower()
    
    # Remover pontuaÃ§Ã£o
    text = re.sub(r'[^\w\s]', '', text)
    
    # Remover stopwords
    stopwords = ['de', 'da', 'do', 'em', 'a', 'o', ...]
    words = [w for w in text.split() if w not in stopwords]
    
    # Stemming (reduzir palavras Ã  raiz)
    stemmer = PortugueseStemmer()
    words = [stemmer.stem(w) for w in words]
    
    return ' '.join(words)

# 2. VetorizaÃ§Ã£o TF-IDF
vectorizer = TfidfVectorizer(
    max_features=5000,
    ngram_range=(1, 3),    # Unigrams, bigrams, trigrams
    min_df=2,               # MÃ­nimo 2 documentos
    max_df=0.8,             # MÃ¡ximo 80% dos docs
)

# 3. ClassificaÃ§Ã£o
classifier = MultinomialNB(alpha=1.0)
```

#### Exemplos de CategorizaÃ§Ã£o

```python
exemplos = [
    {
        'descricao': 'UBER *TRIP',
        'categoria_predicted': 'Transporte',
        'confidence': 0.95
    },
    {
        'descricao': 'RESTAURANTE JAPONÃŠS',
        'categoria_predicted': 'AlimentaÃ§Ã£o',
        'confidence': 0.92
    },
    {
        'descricao': 'NETFLIX.COM',
        'categoria_predicted': 'Lazer',
        'confidence': 0.88
    },
    {
        'descricao': 'FARMACIA DROGASIL',
        'categoria_predicted': 'SaÃºde',
        'confidence': 0.93
    }
]
```

#### DicionÃ¡rio de Termos

**Criado automaticamente a partir dos dados:**
```python
category_keywords = {
    'AlimentaÃ§Ã£o': [
        'restaurante', 'lanche', 'padaria', 'mercado', 'supermercado',
        'ifood', 'delivery', 'pizza', 'burger', 'cafe', ...
    ],
    'Transporte': [
        'uber', 'taxi', 'gasolina', 'combustivel', 'estacionamento',
        '99', 'posto', 'onibus', 'metro', ...
    ],
    'Moradia': [
        'aluguel', 'condominio', 'agua', 'luz', 'energia',
        'gas', 'iptu', 'internet', ...
    ],
    # ... outras categorias
}
```

#### Threshold de ConfianÃ§a

```python
def classify_with_confidence(descricao, threshold=0.75):
    # PrÃ©-processar
    processed = preprocess_text(descricao)
    
    # Vetorizar
    vector = vectorizer.transform([processed])
    
    # Predizer com probabilidades
    proba = classifier.predict_proba(vector)[0]
    top_idx = proba.argmax()
    confidence = proba[top_idx]
    
    if confidence >= threshold:
        # Aplicar automaticamente
        return {
            'categoria': classes[top_idx],
            'confidence': confidence,
            'auto_applied': True
        }
    else:
        # Sugerir top 3
        top_3_idx = proba.argsort()[-3:][::-1]
        return {
            'sugestoes': [
                {'categoria': classes[i], 'confidence': proba[i]}
                for i in top_3_idx
            ],
            'auto_applied': False
        }
```

#### Performance

| MÃ©trica | Valor |
|---------|-------|
| Accuracy | 88% |
| Macro F1 | 85% |
| Weighted F1 | 87% |

**Por Categoria (F1-Score):**
- AlimentaÃ§Ã£o: 91%
- Transporte: 89%
- Moradia: 86%
- SaÃºde: 84%
- EducaÃ§Ã£o: 82%
- Lazer: 87%
- Outros: 75%

#### Melhorias Futuras com BERT

```python
# Arquitetura planejada
model = TFBertForSequenceClassification.from_pretrained(
    'neuralmind/bert-base-portuguese-cased',
    num_labels=num_categories
)

# Fine-tuning
# - Transfer learning do BERT portuguÃªs
# - Esperado: +5-10% accuracy
# - Trade-off: 10x mais lento que Naive Bayes
```

---

### 4. Otimizador de OrÃ§amento

#### Objetivo
Sugerir alocaÃ§Ã£o Ã³tima de recursos baseado em prioridades e restriÃ§Ãµes.

#### Algoritmo
**Linear Programming** com Scipy

#### FormulaÃ§Ã£o do Problema

**VariÃ¡veis de DecisÃ£o:**
```
x_i = valor alocado para categoria i
```

**FunÃ§Ã£o Objetivo:**
```
Maximizar: Î£ (prioridade_i * x_i)
```

**RestriÃ§Ãµes:**
```
1. OrÃ§amento total: Î£ x_i <= receita_mensal
2. Despesas essenciais: x_essencial >= valor_minimo
3. Limites por categoria: x_i <= limite_categoria_i
4. NÃ£o-negatividade: x_i >= 0
```

#### ImplementaÃ§Ã£o

```python
from scipy.optimize import linprog

def optimize_budget(receita, categorias, prioridades, limites):
    """
    Otimiza alocaÃ§Ã£o de orÃ§amento
    """
    n = len(categorias)
    
    # FunÃ§Ã£o objetivo (negativa para maximizar)
    c = [-p for p in prioridades]
    
    # RestriÃ§Ãµes de desigualdade (Ax <= b)
    A_ub = []
    b_ub = []
    
    # RestriÃ§Ã£o 1: Soma <= receita
    A_ub.append([1] * n)
    b_ub.append(receita)
    
    # RestriÃ§Ã£o 2: Cada categoria <= limite
    for i in range(n):
        constraint = [0] * n
        constraint[i] = 1
        A_ub.append(constraint)
        b_ub.append(limites[i])
    
    # RestriÃ§Ãµes de igualdade (essenciais)
    A_eq = []
    b_eq = []
    for i, cat in enumerate(categorias):
        if cat['essencial']:
            constraint = [0] * n
            constraint[i] = 1
            A_eq.append(constraint)
            b_eq.append(cat['minimo'])
    
    # Resolver
    result = linprog(
        c=c,
        A_ub=A_ub,
        b_ub=b_ub,
        A_eq=A_eq if A_eq else None,
        b_eq=b_eq if b_eq else None,
        bounds=(0, None),
        method='highs'
    )
    
    if result.success:
        return {
            'alocacao': dict(zip(categorias, result.x)),
            'total': sum(result.x),
            'economia': receita - sum(result.x),
            'otimalidade': 'Ã“timo'
        }
    else:
        return {'erro': 'Problema infactÃ­vel'}
```

#### Exemplo de Uso

```python
receita = 5000.00

categorias = [
    {'nome': 'Moradia', 'essencial': True, 'minimo': 1500},
    {'nome': 'AlimentaÃ§Ã£o', 'essencial': True, 'minimo': 800},
    {'nome': 'Transporte', 'essencial': True, 'minimo': 400},
    {'nome': 'SaÃºde', 'essencial': True, 'minimo': 300},
    {'nome': 'Lazer', 'essencial': False, 'minimo': 0},
    {'nome': 'Investimento', 'essencial': False, 'minimo': 0},
]

prioridades = [10, 9, 8, 9, 5, 7]  # 1-10
limites = [1500, 1000, 500, 500, 400, 1000]

resultado = optimize_budget(receita, categorias, prioridades, limites)

# Output:
{
    'alocacao': {
        'Moradia': 1500,
        'AlimentaÃ§Ã£o': 800,
        'Transporte': 400,
        'SaÃºde': 500,
        'Lazer': 300,
        'Investimento': 500
    },
    'total': 4000,
    'economia': 1000,
    'otimalidade': 'Ã“timo'
}
```

---

## Pipeline de Dados

### ETL (Extract, Transform, Load)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Extract   â”‚ â† PostgreSQL (transaÃ§Ãµes, usuÃ¡rios)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚  Transform  â”‚ â† Limpeza, Feature Engineering, NormalizaÃ§Ã£o
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚    Load     â”‚ â†’ Feature Store (Parquet, Redis)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Feature Store

**Estrutura:**
```
feature_store/
â”œâ”€â”€ raw_features/
â”‚   â”œâ”€â”€ transactions_daily.parquet
â”‚   â”œâ”€â”€ user_profile.parquet
â”‚   â””â”€â”€ categories.parquet
â”œâ”€â”€ engineered_features/
â”‚   â”œâ”€â”€ temporal_features.parquet
â”‚   â”œâ”€â”€ aggregated_features.parquet
â”‚   â””â”€â”€ behavioral_features.parquet
â””â”€â”€ model_features/
    â”œâ”€â”€ predictor_features_v1.parquet
    â”œâ”€â”€ detector_features_v1.parquet
    â””â”€â”€ classifier_features_v1.parquet
```

**BenefÃ­cios:**
- Reuso de features entre modelos
- Versionamento
- ConsistÃªncia treino/inferÃªncia
- Performance (prÃ©-computado)

### Data Versioning (DVC)

```bash
# Versionar dados
dvc add data/raw/transactions.csv
dvc add data/processed/features.parquet

# Versionar modelos
dvc add models/expense_predictor_v2.pkl

# Commit
git add data.dvc models.dvc
git commit -m "Update features and retrain model"
git push

dvc push
```

---

## Feature Engineering

### CriaÃ§Ã£o de Features

#### 1. Features Temporais

```python
def create_temporal_features(df):
    """
    Extrai features de datas
    """
    df['mes'] = df['data'].dt.month
    df['ano'] = df['data'].dt.year
    df['dia_mes'] = df['data'].dt.day
    df['dia_semana'] = df['data'].dt.dayofweek
    df['semana_ano'] = df['data'].dt.isocalendar().week
    df['quinzena'] = (df['dia_mes'] > 15).astype(int) + 1
    df['trimestre'] = df['data'].dt.quarter
    
    # Flags
    df['is_inicio_mes'] = (df['dia_mes'] <= 5).astype(int)
    df['is_fim_mes'] = (df['dia_mes'] >= 25).astype(int)
    df['is_fim_semana'] = df['dia_semana'].isin([5, 6]).astype(int)
    
    # CÃ­clicas (capturar periodicidade)
    df['mes_sin'] = np.sin(2 * np.pi * df['mes'] / 12)
    df['mes_cos'] = np.cos(2 * np.pi * df['mes'] / 12)
    df['dia_semana_sin'] = np.sin(2 * np.pi * df['dia_semana'] / 7)
    df['dia_semana_cos'] = np.cos(2 * np.pi * df['dia_semana'] / 7)
    
    return df
```

#### 2. Features Agregadas

```python
def create_aggregated_features(df, window_sizes=[7, 30, 90, 365]):
    """
    Features de janelas temporais
    """
    for window in window_sizes:
        # MÃ©dia mÃ³vel
        df[f'valor_mean_{window}d'] = (
            df.groupby('usuario_id')['valor']
            .rolling(window=window, min_periods=1)
            .mean()
            .reset_index(0, drop=True)
        )
        
        # Desvio padrÃ£o
        df[f'valor_std_{window}d'] = (
            df.groupby('usuario_id')['valor']
            .rolling(window=window, min_periods=1)
            .std()
            .reset_index(0, drop=True)
        )
        
        # MÃ¡ximo e mÃ­nimo
        df[f'valor_max_{window}d'] = (
            df.groupby('usuario_id')['valor']
            .rolling(window=window, min_periods=1)
            .max()
            .reset_index(0, drop=True)
        )
        
        # Contagem
        df[f'count_{window}d'] = (
            df.groupby('usuario_id')['valor']
            .rolling(window=window, min_periods=1)
            .count()
            .reset_index(0, drop=True)
        )
        
        # TendÃªncia (slope da regressÃ£o linear)
        df[f'trend_{window}d'] = (
            df.groupby('usuario_id')['valor']
            .rolling(window=window, min_periods=2)
            .apply(lambda x: np.polyfit(range(len(x)), x, 1)[0] if len(x) > 1 else 0)
            .reset_index(0, drop=True)
        )
    
    return df
```

#### 3. Features Comportamentais

```python
def create_behavioral_features(df):
    """
    PadrÃµes de comportamento do usuÃ¡rio
    """
    # FrequÃªncia de categorias
    df['categoria_freq_mes'] = (
        df.groupby(['usuario_id', 'categoria_id', 'mes'])['id']
        .transform('count')
    )
    
    # Percentual de despesas parceladas
    df['pct_parceladas'] = (
        df.groupby(['usuario_id', 'mes'])['parcela_total']
        .transform(lambda x: (x > 1).sum() / len(x))
    )
    
    # DiversificaÃ§Ã£o de categorias
    df['num_categorias_mes'] = (
        df.groupby(['usuario_id', 'mes'])['categoria_id']
        .transform('nunique')
    )
    
    # Velocidade de gasto (R$/dia)
    df['velocidade_gasto'] = (
        df.groupby(['usuario_id', 'mes'])['valor']
        .transform('sum') / 30
    )
    
    # Regularidade (std do intervalo entre transaÃ§Ãµes)
    df['regularidade'] = (
        df.groupby('usuario_id')['data']
        .diff()
        .dt.days
        .rolling(window=10)
        .std()
    )
    
    return df
```

#### 4. Features de InteraÃ§Ã£o

```python
def create_interaction_features(df):
    """
    CombinaÃ§Ãµes de features
    """
    # Valor x Dia do mÃªs
    df['valor_x_dia'] = df['valor'] * df['dia_mes']
    
    # Categoria x Dia da semana
    df['cat_dow_interaction'] = (
        df['categoria_id'].astype(str) + '_' + 
        df['dia_semana'].astype(str)
    )
    
    # Percentual da receita
    df['pct_receita'] = df['valor'] / df['receita_mensal']
    
    # Desvio da mÃ©dia da categoria
    df['desvio_categoria'] = (
        df['valor'] - 
        df.groupby('categoria_id')['valor'].transform('mean')
    )
    
    return df
```

### Feature Selection

```python
from sklearn.feature_selection import SelectKBest, mutual_info_regression

def select_features(X, y, k=50):
    """
    Seleciona top K features mais relevantes
    """
    # Mutual Information
    selector = SelectKBest(mutual_info_regression, k=k)
    selector.fit(X, y)
    
    # Features selecionadas
    selected_features = X.columns[selector.get_support()].tolist()
    
    # Feature importance scores
    scores = pd.DataFrame({
        'feature': X.columns,
        'score': selector.scores_
    }).sort_values('score', ascending=False)
    
    return selected_features, scores
```

---

## Treinamento e AvaliaÃ§Ã£o

### Split de Dados

```python
def temporal_train_test_split(df, test_size=0.2):
    """
    Split temporal (preserva ordem cronolÃ³gica)
    """
    # Ordenar por data
    df = df.sort_values('data')
    
    # Split
    split_idx = int(len(df) * (1 - test_size))
    train = df.iloc[:split_idx]
    test = df.iloc[split_idx:]
    
    return train, test

# ValidaÃ§Ã£o cruzada temporal
from sklearn.model_selection import TimeSeriesSplit

tscv = TimeSeriesSplit(n_splits=5)
for train_idx, val_idx in tscv.split(X):
    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]
    # Treinar e avaliar
```

### HiperparÃ¢metro Tuning

```python
from sklearn.model_selection import RandomizedSearchCV

# Grid de hiperparÃ¢metros
param_distributions = {
    'n_estimators': [100, 200, 300, 500],
    'max_depth': [10, 15, 20, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['sqrt', 'log2', None],
}

# Random Search
random_search = RandomizedSearchCV(
    RandomForestRegressor(random_state=42),
    param_distributions=param_distributions,
    n_iter=50,
    cv=TimeSeriesSplit(n_splits=3),
    scoring='neg_mean_absolute_error',
    n_jobs=-1,
    verbose=2
)

random_search.fit(X_train, y_train)

best_model = random_search.best_estimator_
print("Melhores hiperparÃ¢metros:", random_search.best_params_)
```

### MÃ©tricas de AvaliaÃ§Ã£o

```python
from sklearn.metrics import (
    mean_absolute_error,
    mean_squared_error,
    r2_score,
    mean_absolute_percentage_error
)

def evaluate_model(y_true, y_pred):
    """
    Calcula mÃºltiplas mÃ©tricas
    """
    metrics = {
        'MAE': mean_absolute_error(y_true, y_pred),
        'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),
        'R2': r2_score(y_true, y_pred),
        'MAPE': mean_absolute_percentage_error(y_true, y_pred) * 100,
    }
    
    # MÃ©tricas customizadas
    residuals = y_true - y_pred
    metrics['Med_AE'] = np.median(np.abs(residuals))
    metrics['Std_Residuals'] = np.std(residuals)
    
    return metrics
```

### Experiment Tracking (MLflow)

```python
import mlflow

mlflow.set_experiment("expense_prediction")

with mlflow.start_run():
    # Log hiperparÃ¢metros
    mlflow.log_params(best_params)
    
    # Treinar
    model.fit(X_train, y_train)
    
    # Avaliar
    y_pred = model.predict(X_test)
    metrics = evaluate_model(y_test, y_pred)
    
    # Log mÃ©tricas
    mlflow.log_metrics(metrics)
    
    # Log modelo
    mlflow.sklearn.log_model(model, "model")
    
    # Log artifacts
    mlflow.log_artifact("feature_importance.png")
```

---

## Deployment e Serving

### Model Registry

```python
# Registrar modelo no MLflow
mlflow.sklearn.log_model(
    sk_model=model,
    artifact_path="expense_predictor",
    registered_model_name="ExpensePredictor"
)

# Promover para produÃ§Ã£o
client = mlflow.tracking.MlflowClient()
client.transition_model_version_stage(
    name="ExpensePredictor",
    version=3,
    stage="Production"
)
```

### API Endpoints

```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel

app = FastAPI(title="Financial AI API")

class PredictionRequest(BaseModel):
    usuario_id: int
    mes: int
    ano: int
    features: dict

@app.post("/predict/expenses")
async def predict_expenses(request: PredictionRequest):
    """
    Prediz gastos futuros
    """
    try:
        # Carregar modelo
        model = load_model("ExpensePredictor", stage="Production")
        
        # Preparar features
        X = prepare_features(request)
        
        # Predizer
        prediction = model.predict(X)
        confidence = model.predict_proba(X) if hasattr(model, 'predict_proba') else None
        
        return {
            "usuario_id": request.usuario_id,
            "mes": request.mes,
            "ano": request.ano,
            "valor_previsto": float(prediction[0]),
            "confianca": float(confidence[0]) if confidence else None,
            "model_version": get_model_version()
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

### Caching de PrediÃ§Ãµes

```python
from functools import lru_cache
import redis

redis_client = redis.Redis(host='localhost', port=6379, db=0)

def predict_with_cache(usuario_id, mes, ano):
    """
    PrediÃ§Ã£o com cache Redis
    """
    cache_key = f"prediction:{usuario_id}:{ano}-{mes:02d}"
    
    # Tentar cache
    cached = redis_client.get(cache_key)
    if cached:
        return json.loads(cached)
    
    # Calcular
    prediction = model.predict(...)
    
    # Armazenar cache (TTL 24h)
    redis_client.setex(
        cache_key,
        86400,  # 24 horas
        json.dumps(prediction)
    )
    
    return prediction
```

---

## Monitoramento e Retreinamento

### Monitoramento de Performance

```python
def monitor_model_performance():
    """
    Monitora degradaÃ§Ã£o do modelo
    """
    # Buscar prediÃ§Ãµes recentes
    predictions = get_recent_predictions(days=30)
    
    # Calcular mÃ©tricas
    mae_atual = mean_absolute_error(
        predictions['valor_real'],
        predictions['valor_previsto']
    )
    
    # Comparar com baseline
    mae_baseline = get_baseline_metric('MAE')
    degradation = (mae_atual - mae_baseline) / mae_baseline
    
    # Alertar se degradaÃ§Ã£o > 15%
    if degradation > 0.15:
        send_alert(
            f"Model performance degraded by {degradation:.1%}. "
            f"Current MAE: {mae_atual:.2f}, Baseline: {mae_baseline:.2f}"
        )
        trigger_retraining()
```

### Data Drift Detection

```python
from scipy.stats import ks_2samp

def detect_data_drift(feature, train_dist, prod_dist, threshold=0.05):
    """
    Detecta mudanÃ§a na distribuiÃ§Ã£o de features
    """
    # Teste Kolmogorov-Smirnov
    statistic, p_value = ks_2samp(train_dist, prod_dist)
    
    drift_detected = p_value < threshold
    
    if drift_detected:
        logger.warning(
            f"Data drift detected in feature '{feature}'. "
            f"KS statistic: {statistic:.4f}, p-value: {p_value:.4f}"
        )
    
    return drift_detected
```

### Retreinamento AutomÃ¡tico

```python
def auto_retrain_pipeline():
    """
    Pipeline de retreinamento
    """
    # 1. Verificar triggers
    should_retrain = (
        check_performance_degradation() or
        check_data_drift() or
        check_scheduled_retrain()
    )
    
    if not should_retrain:
        return
    
    # 2. Extrair novos dados
    new_data = extract_recent_data()
    
    # 3. Feature engineering
    features = engineer_features(new_data)
    
    # 4. Treinar novo modelo
    new_model = train_model(features)
    
    # 5. Avaliar
    metrics = evaluate_model(new_model)
    
    # 6. A/B test
    if metrics['MAE'] < current_model_mae:
        # Substituir modelo
        deploy_model(new_model, version='v_new')
        logger.info(f"Model retrained successfully. New MAE: {metrics['MAE']:.2f}")
    else:
        logger.warning("New model performance worse than current. Keeping current model.")
```

**Schedule:**
- DiÃ¡rio: Monitoramento de mÃ©tricas
- Semanal: DetecÃ§Ã£o de data drift
- Mensal: Retreinamento completo
- Ad-hoc: Trigger por performance degradation

---

## APIs de IA

### Endpoints DisponÃ­veis

#### 1. PrevisÃ£o de Despesas

```
POST /api/ia/predict/expenses
```

**Request:**
```json
{
  "usuario_id": 123,
  "mes": 2,
  "ano": 2026,
  "incluir_breakdown": true
}
```

**Response:**
```json
{
  "valor_previsto": 3250.50,
  "intervalo_confianca": {
    "min": 2900.00,
    "max": 3600.00
  },
  "confianca": 0.85,
  "breakdown_categorias": [
    {"categoria": "AlimentaÃ§Ã£o", "valor": 800.00},
    {"categoria": "Transporte", "valor": 600.00},
    {"categoria": "Moradia", "valor": 1500.00}
  ],
  "comparacao_mes_anterior": {
    "valor": 3100.00,
    "variacao": "+4.9%"
  }
}
```

#### 2. DetecÃ§Ã£o de Anomalias

```
POST /api/ia/detect/anomaly
```

**Request:**
```json
{
  "despesa_id": 456,
  "usuario_id": 123,
  "valor": 5000.00,
  "categoria_id": 7,
  "data": "2026-01-30",
  "descricao": "Compra eletrÃ´nicos"
}
```

**Response:**
```json
{
  "is_anomaly": true,
  "score": 0.87,
  "confianca": "alta",
  "razoes": [
    "Valor 5x acima da mÃ©dia histÃ³rica",
    "Categoria raramente usada (Ãºltima vez hÃ¡ 8 meses)",
    "Valor representa 62% da receita mensal"
  ],
  "sugestoes": [
    "Verificar se a transaÃ§Ã£o foi autorizada",
    "Considerar parcelar o valor",
    "Avaliar impacto nas metas financeiras"
  ]
}
```

#### 3. CategorizaÃ§Ã£o AutomÃ¡tica

```
POST /api/ia/classify/category
```

**Request:**
```json
{
  "descricao": "UBER *TRIP 12345",
  "valor": 25.50,
  "data": "2026-01-30"
}
```

**Response:**
```json
{
  "categoria_sugerida": {
    "id": 2,
    "nome": "Transporte",
    "confianca": 0.96
  },
  "alternativas": [
    {"id": 6, "nome": "Lazer", "confianca": 0.03},
    {"id": 8, "nome": "Outros", "confianca": 0.01}
  ],
  "aplicado_automaticamente": true
}
```

#### 4. RecomendaÃ§Ãµes de Economia

```
GET /api/ia/recommendations/{usuario_id}
```

**Response:**
```json
{
  "recomendacoes": [
    {
      "tipo": "substituicao",
      "titulo": "Reduza gastos com delivery",
      "descricao": "VocÃª gasta R$ 400/mÃªs com delivery. Cozinhar em casa 3x/semana economizaria ~R$ 150.",
      "economia_mensal": 150.00,
      "economia_anual": 1800.00,
      "facilidade": "media",
      "impacto": "medio",
      "prioridade": 8
    },
    {
      "tipo": "eliminacao",
      "titulo": "Assinatura sem uso: Netflix",
      "descricao": "Assinatura de R$ 45/mÃªs nÃ£o utilizada nos Ãºltimos 60 dias.",
      "economia_mensal": 45.00,
      "economia_anual": 540.00,
      "facilidade": "facil",
      "impacto": "baixo",
      "prioridade": 6,
      "acao_sugerida": "Cancelar assinatura"
    }
  ],
  "economia_total_potencial": {
    "mensal": 195.00,
    "anual": 2340.00
  }
}
```

#### 5. Insights Personalizados

```
GET /api/ia/insights/{usuario_id}
```

**Response:**
```json
{
  "insights": [
    {
      "tipo": "padrao",
      "titulo": "PadrÃ£o de gasto identificado",
      "descricao": "VocÃª tende a gastar 30% mais nas primeiras semanas do mÃªs.",
      "dados_suporte": {
        "primeira_quinzena_media": 1950.00,
        "segunda_quinzena_media": 1350.00
      },
      "sugestao": "Considere distribuir gastos mais uniformemente ao longo do mÃªs."
    },
    {
      "tipo": "oportunidade",
      "titulo": "Meta de viagem alcanÃ§Ã¡vel",
      "descricao": "Com economia de R$ 300/mÃªs, vocÃª atingiria sua meta de viagem em 8 meses.",
      "meta_id": 7,
      "economia_necessaria": 300.00,
      "prazo_estimado": "8 meses"
    }
  ]
}
```

---

## Casos de Uso

### 1. Onboarding Inteligente

**CenÃ¡rio:** Novo usuÃ¡rio sem histÃ³rico

**SoluÃ§Ã£o IA:**
```python
def smart_onboarding(usuario_id):
    # 1. Classificar perfil baseado em questionÃ¡rio
    perfil = classify_user_profile(
        renda=input_renda,
        idade=input_idade,
        dependentes=input_dependentes
    )
    
    # 2. Sugerir orÃ§amento baseado em perfis similares
    orcamento_sugerido = recommend_budget_from_similar_users(perfil)
    
    # 3. Criar metas realistas
    metas_sugeridas = suggest_realistic_goals(perfil, orcamento_sugerido)
    
    return {
        'perfil': perfil,
        'orcamento': orcamento_sugerido,
        'metas': metas_sugeridas
    }
```

### 2. Alertas Proativos

**CenÃ¡rio:** UsuÃ¡rio prÃ³ximo de estourar orÃ§amento

**SoluÃ§Ã£o IA:**
```python
def proactive_budget_alert(usuario_id, mes_atual):
    # Prever gastos restantes do mÃªs
    dias_restantes = days_until_month_end()
    gasto_previsto = predict_remaining_expenses(usuario_id, dias_restantes)
    
    # Comparar com orÃ§amento
    orcamento_mes = get_monthly_budget(usuario_id)
    gasto_atual = get_current_month_expenses(usuario_id)
    
    if gasto_atual + gasto_previsto > orcamento_mes:
        # Enviar alerta
        send_alert(
            usuario_id,
            tipo='ALERTA_ORCAMENTO',
            mensagem=f"AtenÃ§Ã£o! Baseado no seu padrÃ£o de gastos, vocÃª pode "
                    f"exceder o orÃ§amento em R$ {excesso:.2f} este mÃªs.",
            sugestoes=generate_cost_cutting_suggestions(usuario_id)
        )
```

### 3. Assistente de Metas

**CenÃ¡rio:** UsuÃ¡rio quer atingir meta mais rÃ¡pido

**SoluÃ§Ã£o IA:**
```python
def goal_acceleration_plan(meta_id):
    meta = get_meta(meta_id)
    
    # Analisar padrÃ£o de aportes
    aportes_historico = get_aportes_history(meta_id)
    media_aporte = np.mean(aportes_historico)
    
    # Identificar oportunidades de economia
    economia_potencial = identify_savings_opportunities(meta.usuario_id)
    
    # Calcular novo prazo se redirecionar economias
    novo_aporte_mensal = media_aporte + sum(economia_potencial)
    novo_prazo = calculate_new_deadline(
        meta.valor_objetivo,
        meta.valor_atual,
        novo_aporte_mensal
    )
    
    return {
        'economia_total': sum(economia_potencial),
        'novo_aporte_mensal': novo_aporte_mensal,
        'novo_prazo': novo_prazo,
        'aceleracao': meta.prazo - novo_prazo,
        'acoes': economia_potencial
    }
```

### 4. Planejamento Financeiro Anual

**CenÃ¡rio:** InÃ­cio de ano, usuÃ¡rio quer planejar

**SoluÃ§Ã£o IA:**
```python
def annual_financial_plan(usuario_id, ano):
    # Prever receitas e despesas por mÃªs
    previsoes = []
    for mes in range(1, 13):
        prev = predict_expenses(usuario_id, mes, ano)
        previsoes.append(prev)
    
    # Identificar meses crÃ­ticos
    meses_criticos = identify_tight_months(previsoes)
    
    # Sugerir ajustes
    ajustes = suggest_budget_adjustments(previsoes, meses_criticos)
    
    # Calcular metas atingÃ­veis
    economia_esperada = sum(p['saldo'] for p in previsoes if p['saldo'] > 0)
    metas_sugeridas = suggest_annual_goals(usuario_id, economia_esperada)
    
    return {
        'previsoes_mensais': previsoes,
        'meses_criticos': meses_criticos,
        'ajustes_sugeridos': ajustes,
        'economia_total_prevista': economia_esperada,
        'metas_recomendadas': metas_sugeridas
    }
```

---

## Roadmap de IA

### Fase 1 - MVP âœ… (Atual)
- [x] PrediÃ§Ã£o bÃ¡sica de despesas (Random Forest)
- [x] DetecÃ§Ã£o de anomalias (Isolation Forest)
- [x] CategorizaÃ§Ã£o automÃ¡tica (Naive Bayes)
- [x] Otimizador de orÃ§amento (Linear Programming)

### Fase 2 - Melhorias ğŸš§ (3-6 meses)
- [ ] BERT para categorizaÃ§Ã£o (NLP avanÃ§ado)
- [ ] PrevisÃ£o multi-step (prÃ³ximos 3-6 meses)
- [ ] Recommender system para metas
- [ ] Clustering de perfis de usuÃ¡rios
- [ ] AnÃ¡lise de sentimento em notas/observaÃ§Ãµes

### Fase 3 - Recursos AvanÃ§ados ğŸ“‹ (6-12 meses)
- [ ] Time series forecasting com LSTM/Prophet
- [ ] Explicabilidade de modelos (SHAP, LIME)
- [ ] AutoML para otimizaÃ§Ã£o contÃ­nua
- [ ] Reinforcement Learning para estratÃ©gias de investimento
- [ ] Computer Vision para OCR de recibos

### Fase 4 - AI Agents ğŸ”® (12+ meses)
- [ ] Assistente conversacional (GPT-4/Claude)
- [ ] NegociaÃ§Ã£o automÃ¡tica de contas (API bancÃ¡rias)
- [ ] Planejamento financeiro end-to-end
- [ ] IntegraÃ§Ã£o com Open Finance
- [ ] PrevisÃ£o de eventos de vida (casamento, filho, etc.)

---

## ConsideraÃ§Ãµes Ã‰ticas e Privacidade

### Privacidade

1. **Dados MÃ­nimos**: Usar apenas dados necessÃ¡rios para cada modelo
2. **AnonimizaÃ§Ã£o**: Remover PII em datasets de treino
3. **Consentimento**: Opt-in explÃ­cito para uso de IA
4. **TransparÃªncia**: Explicar decisÃµes de IA ao usuÃ¡rio

### Bias e Fairness

```python
def check_model_fairness(model, X_test, y_test, protected_attr):
    """
    Verifica viÃ©s em grupos protegidos
    """
    from fairlearn.metrics import MetricFrame
    
    metric_frame = MetricFrame(
        metrics=mean_absolute_error,
        y_true=y_test,
        y_pred=model.predict(X_test),
        sensitive_features=X_test[protected_attr]
    )
    
    # Disparidade entre grupos
    disparity = metric_frame.difference()
    
    if disparity > THRESHOLD:
        logger.warning(f"Potential bias detected: {disparity}")
```

### Explicabilidade

```python
import shap

def explain_prediction(model, instance):
    """
    Explica prediÃ§Ã£o individual
    """
    explainer = shap.TreeExplainer(model)
    shap_values = explainer.shap_values(instance)
    
    # Top 5 features que influenciaram
    feature_importance = sorted(
        zip(instance.columns, shap_values[0]),
        key=lambda x: abs(x[1]),
        reverse=True
    )[:5]
    
    return {
        'prediction': model.predict(instance)[0],
        'top_influences': feature_importance
    }
```

---

**Ãšltima AtualizaÃ§Ã£o:** Janeiro 2026  
**VersÃ£o:** 2.0  
**ResponsÃ¡vel:** AI/ML Team
